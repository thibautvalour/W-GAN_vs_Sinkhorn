{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will investigate the ability of Wasserstein-GAN (W-GAN) to approximate the 1-Wasserstein distance. We will begin by formulating the problem and selecting ground truth distributions for which the true optimal transport (OT) distances are known.\n",
    "\n",
    "# Problem Formulation\n",
    "\n",
    "**Objective:** Given two probability distributions P and Q, we want to compute the 1-Wasserstein distance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ground Truth Distributions : \n",
    "We will select a set of ground truth distributions for which the true optimal transport (OT) distances are known. In this case, we will consider 1D and 2D Gaussians.\n",
    "\n",
    "First, let's define a function to generate ground truth Gaussian distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussians(num_distributions, dim):\n",
    "    means = [torch.randn(dim) for _ in range(num_distributions)]\n",
    "    covariances = [torch.randn(dim, dim) for _ in range(num_distributions)]\n",
    "    covariances = [A @ A.T for A in covariances]  # Ensure positive semi-definite\n",
    "    \n",
    "    return means, covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a set of ground truth Gaussian distributions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_distributions = 10\n",
    "dim = 2\n",
    "\n",
    "means, covariances = generate_gaussians(num_distributions, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to compute the true OT distances between these Gaussian distributions. For simplicity, we will compute the distances only between consecutive pairs of distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_ot_distance(mean1, cov1, mean2, cov2):\n",
    "    mean_diff = mean2 - mean1\n",
    "    cov_sum = cov1 + cov2\n",
    "    ot_distance = (torch.sqrt(torch.dot(mean_diff.t(), mean_diff)) \n",
    "                   + torch.trace(cov_sum - 2 * torch.sqrt(torch.sqrt(cov1) @ (torch.sqrt(cov2) @ torch.sqrt(cov1)))))\n",
    "    return ot_distance\n",
    "\n",
    "true_ot_distances = [true_ot_distance(means[i], covariances[i], means[i+1], covariances[i+1]) for i in range(num_distributions - 1)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. W-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 2 of the plan, we will implement the W-GAN model and train the critic on the selected ground truth distributions. Then, we will evaluate the performance of the W-GAN by comparing its approximated Wasserstein distances with the known true OT distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the critic neural network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Critic Loss: -4.875846207141876e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m# Calculate the Wasserstein distance approximation\u001b[39;00m\n\u001b[0;32m     28\u001b[0m real_scores \u001b[39m=\u001b[39m critic(real_data)\n\u001b[1;32m---> 29\u001b[0m fake_scores \u001b[39m=\u001b[39m critic(fake_data)\n\u001b[0;32m     30\u001b[0m wasserstein_approx \u001b[39m=\u001b[39m real_scores\u001b[39m.\u001b[39mmean() \u001b[39m-\u001b[39m fake_scores\u001b[39m.\u001b[39mmean()\n\u001b[0;32m     32\u001b[0m \u001b[39m# Update the critic\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\33646\\Desktop\\ENSAE\\CUTURI_MARCO\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n",
      "File \u001b[1;32mc:\\Users\\33646\\Desktop\\ENSAE\\CUTURI_MARCO\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\33646\\Desktop\\ENSAE\\CUTURI_MARCO\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\33646\\Desktop\\ENSAE\\CUTURI_MARCO\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\33646\\Desktop\\ENSAE\\CUTURI_MARCO\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the critic\n",
    "input_dim = dim\n",
    "critic = Critic(input_dim).to(device)\n",
    "\n",
    "# Set up training parameters\n",
    "epochs = 1000\n",
    "batch_size = 256\n",
    "lr = 1e-4\n",
    "optimizer = optim.Adam(critic.parameters(), lr=lr)\n",
    "clip_value = 0.01\n",
    "n_critic_updates = 5\n",
    "\n",
    "# Define a function to sample from Gaussian distributions\n",
    "def sample_from_gaussian(mean, cov, n_samples):\n",
    "    L = torch.linalg.cholesky(cov)\n",
    "    samples = torch.randn(n_samples, cov.shape[0])\n",
    "    return samples @ L + mean\n",
    "\n",
    "# W-GAN training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_distributions - 1):\n",
    "        for _ in range(n_critic_updates):\n",
    "            # Sample data from both distributions\n",
    "            real_data = sample_from_gaussian(means[i], covariances[i], batch_size).to(device)\n",
    "            fake_data = sample_from_gaussian(means[i+1], covariances[i+1], batch_size).to(device)\n",
    "            \n",
    "            # Calculate the Wasserstein distance approximation\n",
    "            real_scores = critic(real_data)\n",
    "            fake_scores = critic(fake_data)\n",
    "            wasserstein_approx = real_scores.mean() - fake_scores.mean()\n",
    "            \n",
    "            # Update the critic\n",
    "            critic_loss = -wasserstein_approx\n",
    "            optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Clip the critic's weights\n",
    "            for param in critic.parameters():\n",
    "                param.data.clamp_(-clip_value, clip_value)\n",
    "                \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}, Critic Loss: {critic_loss.item()}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the W-GAN is trained, we can evaluate its performance by comparing the approximated Wasserstein distances with the known true OT distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1: True OT Distance = 2.977085828781128, W-GAN Approximated Distance = 0.00043230969458818436\n",
      "Pair 2: True OT Distance = nan, W-GAN Approximated Distance = 0.0001508370041847229\n",
      "Pair 3: True OT Distance = nan, W-GAN Approximated Distance = -0.00026264041662216187\n",
      "Pair 4: True OT Distance = nan, W-GAN Approximated Distance = 0.00019355490803718567\n",
      "Pair 5: True OT Distance = 0.5751767158508301, W-GAN Approximated Distance = -2.5820918381214142e-05\n",
      "Pair 6: True OT Distance = -0.6723374128341675, W-GAN Approximated Distance = 2.878718078136444e-06\n",
      "Pair 7: True OT Distance = nan, W-GAN Approximated Distance = 1.5721656382083893e-05\n",
      "Pair 8: True OT Distance = nan, W-GAN Approximated Distance = -0.0004567587748169899\n",
      "Pair 9: True OT Distance = 4.366995811462402, W-GAN Approximated Distance = 0.0004621315747499466\n"
     ]
    }
   ],
   "source": [
    "def wgan_approximated_distance(mean1, cov1, mean2, cov2, n_samples=1000):\n",
    "    real_data = sample_from_gaussian(mean1, cov1, n_samples).to(device)\n",
    "    fake_data = sample_from_gaussian(mean2, cov2, n_samples).to(device)\n",
    "    \n",
    "    real_scores = critic(real_data)\n",
    "    fake_scores = critic(fake_data)\n",
    "    \n",
    "    return real_scores.mean() - fake_scores.mean()\n",
    "\n",
    "wgan_distances = [wgan_approximated_distance(means[i], covariances[i], means[i+1], covariances[i+1]) for i in range(num_distributions - 1)]\n",
    "\n",
    "# Compare W-GAN approximated distances with true OT distances\n",
    "for i in range(num_distributions - 1):\n",
    "    print(f'Pair {i + 1}: True OT Distance = {true_ot_distances[i].item()}, W-GAN Approximated Distance = {wgan_distances[i].item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sinkhorn divergence\n",
    "\n",
    "First, let's define a function to compute the Sinkhorn divergence between two distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_divergence(mu, nu, C, epsilon, n_iter=100):\n",
    "    K = torch.exp(-C / epsilon)\n",
    "    u = torch.ones_like(mu)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        v = nu / (K.T @ u)\n",
    "        u = mu / (K @ v)\n",
    "        \n",
    "    P = torch.diag(u) @ K @ torch.diag(v)\n",
    "    return torch.sum(P * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost matrix between two sets of samples\n",
    "def compute_cost_matrix(X, Y):\n",
    "    X_sq = torch.sum(X ** 2, dim=1, keepdim=True)\n",
    "    Y_sq = torch.sum(Y ** 2, dim=1, keepdim=True).T\n",
    "    XY = X @ Y.T\n",
    "    return X_sq - 2 * XY + Y_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "n_samples = 1000\n",
    "\n",
    "sinkhorn_distances = []\n",
    "\n",
    "for i in range(num_distributions - 1):\n",
    "    X = sample_from_gaussian(means[i], covariances[i], n_samples).to(device)\n",
    "    Y = sample_from_gaussian(means[i+1], covariances[i+1], n_samples).to(device)\n",
    "    \n",
    "    mu = torch.ones(X.shape[0], device=device) / X.shape[0]\n",
    "    nu = torch.ones(Y.shape[0], device=device) / Y.shape[0]\n",
    "    \n",
    "    C = compute_cost_matrix(X, Y)\n",
    "    \n",
    "    sinkhorn_dist = sinkhorn_divergence(mu, nu, C, epsilon)\n",
    "    sinkhorn_distances.append(sinkhorn_dist.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1: True OT Distance = 2.977085828781128, W-GAN Approximated Distance = 0.00043230969458818436, Sinkhorn Distance = nan\n",
      "Pair 2: True OT Distance = nan, W-GAN Approximated Distance = 0.0001508370041847229, Sinkhorn Distance = nan\n",
      "Pair 3: True OT Distance = nan, W-GAN Approximated Distance = -0.00026264041662216187, Sinkhorn Distance = nan\n",
      "Pair 4: True OT Distance = nan, W-GAN Approximated Distance = 0.00019355490803718567, Sinkhorn Distance = nan\n",
      "Pair 5: True OT Distance = 0.5751767158508301, W-GAN Approximated Distance = -2.5820918381214142e-05, Sinkhorn Distance = nan\n",
      "Pair 6: True OT Distance = -0.6723374128341675, W-GAN Approximated Distance = 2.878718078136444e-06, Sinkhorn Distance = nan\n",
      "Pair 7: True OT Distance = nan, W-GAN Approximated Distance = 1.5721656382083893e-05, Sinkhorn Distance = nan\n",
      "Pair 8: True OT Distance = nan, W-GAN Approximated Distance = -0.0004567587748169899, Sinkhorn Distance = nan\n",
      "Pair 9: True OT Distance = 4.366995811462402, W-GAN Approximated Distance = 0.0004621315747499466, Sinkhorn Distance = nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_distributions - 1):\n",
    "    print(f'Pair {i + 1}: True OT Distance = {true_ot_distances[i].item()}, W-GAN Approximated Distance = {wgan_distances[i].item()}, Sinkhorn Distance = {sinkhorn_distances[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
